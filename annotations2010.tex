\documentclass{article}

\usepackage{listings}

\lstset{
    captionpos=b
}
\begin{document}

% TODO:  need a title
\title{Super Awesome Title Goes Here}

\author{Garrett Holmstrom\thanks{Contact: Garrett Holmstrom, email: garrett@cs.umn.edu},
    Dept. of Computer Science and Engineering \\ University of Minnesota
}

% FIXME:  Why doesn't this show up?
\date{}

\maketitle

\begin{abstract}
\begin{quote}
% TODO:  write me
Future site of an abstract for this paper
\end{quote}
\end{abstract}

\section{Introduction}

% TODO:  basic information about TAC-SCM?

\subsection{MinneTAC architecture}

Our TAC-SCM agent, MinneTAC, is a Java application designed to minimize
coupling between its components to support independent work on multiple
lines of research.  To that end we use a component-oriented approach...
% TODO:  finish above

A MinneTAC agent consists of a set of components for each major decision
process in the TAC-SCM game: \textsc{Procurement}, \textsc{Production},
\textsc{Sales}, and \textsc{Shipping}.  All data to be shared
among components are kept in the Repository, which plays the role
of the Blackboard in the \emph{Blackboard} pattern\cite{Busch96}.
The \textsc{Communications} component manages interaction with the game
server.  Finally, the \textsc{Oracle} component contains a large number
of smaller components that maintain models of markets and inventory and
perform analyses and predictions.  Ideally, each of these components
depends solely upon the \textsc{Repository}, which completely separates
major decision processes and allowing researchers to work on them
independently.

% TODO:  hub-and-spoke diagram

Since decision components cannot depend on one another, they communicate
using \emph{evaluations} that are accessible through the various data
elements in the \textsc{Repository}.  Any calculations or analyses that
are performed on \textsc{Repository} data can be encapsulated in the
form of evaluations and made available to all other components via the
\textsc{Repository}.  The \textsc{Oracle} component contains a large
number of configurable \emph{evaluator} classes that perform analyses
on \textsc{Repository} data.

All the major data elements in the \textsc{Repository}, such as RFQs,
offers, components, and so forth, are Evaluable types.  Each Evaluable can
be queried for related Evaluations by passing it the name of the needed
evaluation.  An EvaluationFactory maintains a mapping of Evaluation
names to Evaluator instances, and calls upon Evaluators to produce
Evaluations on demand.  Evaluators can back-chain by requesting other
Evaluations as they attempt to produce their results.  By this means,
an Evaluation may be composed from several other Evaluations that are
in turn generated by their own Evaluators.

The resulting \textsc{Oracle} component is essentially a framework for
a set of small, configurable sub-components from which other components
can request analyses and predictions.  Most of these sub-components
are Evaluators, though other types also exist.  The \textsc{Oracle}
itself merely uses its configuration data to create and configure
instances of Evaluators and other subclasses of ConfiguredObject.
\emph{ConfiguredObject} is an abstract class whose instances have names
and some ability to configure themselves, given an appropriate clause from
a XML configuration file.  The \textsc{Oracle} creates ConfiguredObject
instances and keeps track of them by mapping their names as given in
the configuration file to instances.

% TODO:  UML diagram?

During initialization, the \textsc{Oracle} processes a configuration
clause that specifies which instances to create, and for each such
instance, what name to give it, the values of any parameters necessary to
configure it, and the names of any instances it queries for input data.
Code listing~\ref{lst:xconf-simpleprice} illustrates such a configuration
clause for an evaluator with one parameter and four inputs.
\\  % FIXME:  text below ends up too wide for the page
{\small
\begin{lstlisting}[language={XML},frame={single},belowcaptionskip=0.4cm,
label={lst:xconf-simpleprice},caption={Configuration clause for a price
evaluator that uses one parameter and several inputs}]
<evaluator
    class="edu.umn.cs.tac.oracle.eval.SimplePriceEvaluator"
    name="simple-price">
    <parameters price-probability-exponent="1.0"/>
</evaluator>
<graph source="simple-price">
    <quantity source="customer-quantity"/>
    <effective-demand source="effective-demand"/>
    <allocation source="allocation"/>
    <regression source="probability-of-acceptance"/>
</graph>
\end{lstlisting}
}

Taken together, a set of Evaluators and other ConfiguredObjects comprises
a directed graph of modular components that constitute the majority
of the functional code used in a MinneTAC agent workflow.  MinneTAC's
modular architecture allows one to easily substitute components for one
another for comparative purposes or to create an entirely new set of
components with only minimal knowledge of how the agent works internally.
Additionally, using configuration files to define graphs of components
allows developers to add new components without disrupting other
developers' configurations.

\section{Usability challenges}

MinneTAC Evaluators gather input data of specific types from zero or
more other Evaluators, then use these data to produce one output of a
given type.  In addition to having a specific set of inputs and outputs,
Evaluators frequently have configuration parameters that affect how they
behave.  Common parameters include exponents and probability thresholds.
Creating an agent workflow involves choosing a set of Sink components
from which the main decision-making components request data, choosing
appropriate graphs of Evaluators that generate the data the chosen Sinks
require, and finally configuring the individual Evaluators.~\cite{Collins08TR}

Historically, MinneTAC has proved itself difficult to configure because of
how complex agent workflows, which tend to have graphs of over one hundred
evaluators, tend to be.  This makes using the agent much less accessible
to users who are less familiar with every available Evaluator.  Adding a
component to a particular part of an agent workflow configuration requires
one to choose among hundreds of Evaluators, most of which have inputs
or outputs that are incompatible with the graph location in question.

Gil et al.~\cite{gil2010wings} identify a key issue that workflow
designers face: tracking and checking input and output data types
becomes onerous as the size of systems or the number of available
components increase.  We aim to simplify MinneTAC's configuration
process by creating a graphical editor for agent workflow configurations.
However, a graphical editor alone does nothing to fix the problem arising
from the sheer number of Evaluators one has to choose from and thus
be familiar with.  By taking context into account and only displaying
Evaluators whose input or output data types are compatible with the
Evaluator selected in the editor we require users to be familiar with
only a small subset of all Evaluators to be able to reasonably choose
which Evaluator is appropriate for a given location in the workflow,
we reduce the chances that the agent will run correctly, and we make it
simpler to write new components that are likely to work as intended.

Matching inputs with compatible outputs is a common problem in
scientific workflow and service composition systems.  A number of
existing technologies attempt to solve parts of this problem with a
variety of methods for specifying and processing component metadata as
well as choosing compatible components.  An informal survey of some of
them reveals several common themes as well as important differences.

\subsection{Apache Excalibur}

Apache Excalibur\footnote{http://excalibur.apache.org/} is a
general-purpose framework for building configurable systems out of
independent components.  It is typically used as a foundation for
server software and middleware, such a the Cocoon web application
framework\footnote{http://cocoon.apache.org/}.  In the past MinneTAC
used Excalibur extensively, so as a result MinneTAC's design is heavily
influenced by that of Excalibur~\cite{ecra07}.  Excalibur components
each fulfill specific \emph{roles}, while an Excalibur system is a set
of roles, each realized by a class specified in a configuration file.

When starting, an Excalibur application reads configuration files that
specify roles, classes that satisfy those roles, and configuration
parameters for those classes.  The framework then instantiates the
appropriate classes and invokes each component's interface.

Most of Excalibur's problems in this situation revolve around
its extensive use of configuration files and framework methods for
communications and inversion of control.  Information about what roles
each component fills is kept exclusively within a configuration file that
is separate from the component it represents.  When editing a system
configuration one must cross-reference any changes made with component
code and documentation, and vice versa --- changes to code must be
reflected in system configurations.  Since role and configuration data
are specified independently of the code, Excalibur requires that the
programmer simply assert that they are consistent with one another;
it performs no tests to determine if that is actually the case.

% complexity of the config file is a problem
% the code that uses the configuration is not constrained to use it correctly

\subsection{Enterprise JavaBeans 3.0 and the Spring Framework}
%spring 2.?

The Enterprise JavaBeans (EJB)
specification\footnote{http://java.sun.com/products/ejb/} is one of
several server-side Java EE APIs.  Before API version 3.0, JavaBeans
applications were constructed similarly to Excalibur applications, via
required interfaces and abstract classes specified by the API.  EJB gained
a reputation for introducing complexity without delivering real benefits.

As a result, a counter-movement pushed for ``lighter-weight'' frameworks.
One of its products, the Spring
Framework\footnote{http://www.springsource.org/}, is another open source
% any guarantees that the dependencies are of the correct type?
application framework for Java and .NET applications.  Central to
this framework is its Inversion of Control container, which provides
a means of configuring and managing Java objects using callbacks,
similar to Excalibur.  Where the Spring Framework differs from earlier
versions of EJB is in how its component metadata are specified: rather
than using a configuration file or required API, one adds annotations to
regular Java classes that specify which fields, methods, and classes need
dependency injection.  The Spring Framework proved to be so successful
that the EJB 3.0 specification borrowed so heavily from Spring that it
was nearly completely rewritten.

Specifying component metadata with code annotations links metadata
with the code spatially so they are more likely to remain consistent.
Additionally, in Java 6.0 such code annotations are easy to parse and
use at runtime.
% FIXME:  What more can we add?

% This sort of thing makes things more like a dynamic language, since the compiler can't do type checking and the programmer simply has to assert that dependencies implement the right interfaces
%
% We have this problem as well, but we use semantics to provide a degree of assurance that types are compatible at configuration time

\subsection{WINGS}

% we have more like a pipe and filter architecture that makes system design using semantics easier
% wings is a much more generalized system that solves a different problem:  linking applications together.  Since we don't always have access to the source code in that scenario we can't rely on internal annotations.
% The way we're doing things requires that we either have pre-annotated classes or have the ability to add them to the source code.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scraps --- bits of text that don't yet have homes} % TODO:  remove this

We accomplish this by creating an ontological class with Web Ontology Language Description Logic (OWL DL)\footnote{http://www.w3.org/TR/owl2-overview/} for each Evaluator that describes its inputs' and output's data types, and then determining which evaluators are compatible with one another's inputs and outputs by querying the ontology.  Given correct ontologies, this additionally gives us a way to programmatically check configurations for data type consistency.

For this scheme to work one needs to define an ontological class for each Evaluator.  With our previous code base this was impossible to do programmatically because Java only had knowledge of its native data types, which are less specific than the domain data types needed by the ontology.  Since domain information was specified solely in the documentation, writing these classes was strictly a manual process.

Each Evaluator thus has information about its input and output data types defined separately in five locations that have to be kept consistent manually:  its documentation, its ontology, its configuration code, its core logic, and in system configurations.  As a result, these tend to diverge from one another over time as they are not kept consistent automatically.

In the past Evaluators had to individually devote code to reading their own configuration and input data.  This is because evaluators have individualized sets of inputs, making the code for fetching input data difficult to factor out.  As a result, many evaluators contain duplicate input-fetching code that often dwarfs their core logic in size.  One might find a given evaluator using any of several generations of input-gathering APIs or possibly even processing their own configuration clauses.

\end{document}
